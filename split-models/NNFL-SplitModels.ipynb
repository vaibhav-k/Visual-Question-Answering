{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VCseE72qvotB",
    "nbpresent": {
     "id": "4158928c-f65f-4395-bd4f-97cd737871b4"
    }
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 27129,
     "status": "ok",
     "timestamp": 1542756452577,
     "user": {
      "displayName": "Deepesh Makhijani",
      "photoUrl": "https://lh6.googleusercontent.com/-yTXHk5B3Bjs/AAAAAAAAAAI/AAAAAAAAAMQ/z9m1NPmwROo/s64/photo.jpg",
      "userId": "00947495800042339933"
     },
     "user_tz": -330
    },
    "id": "g18W-XnFvotD",
    "nbpresent": {
     "id": "2517fa70-c6bb-425f-b117-74131ed21901"
    },
    "outputId": "5968762a-d55c-40cf-8c5e-d7fe970248c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import cv2 as cv\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Embedding, \\\n",
    "    LSTM, Bidirectional, Lambda, concatenate, Add, Concatenate\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization, regularizers\n",
    "from keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M_bYdX5oCQMv",
    "nbpresent": {
     "id": "7175c3e4-c0f2-43c7-a41e-5cc74673d2b5"
    }
   },
   "source": [
    "### Get Data Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O5GBE_PkqcQp",
    "nbpresent": {
     "id": "ef1fd39c-f049-421d-adae-b47a793188e2"
    }
   },
   "outputs": [],
   "source": [
    "# Unpickling JSON data\n",
    "\n",
    "answers_pkl = open(\"answers.pkl\",\"rb\")\n",
    "answers = pickle.load(answers_pkl)\n",
    "answers_pkl.close()\n",
    "\n",
    "questions_pkl = open(\"questions.pkl\",\"rb\")\n",
    "questions = pickle.load(questions_pkl)\n",
    "questions_pkl.close()\n",
    "\n",
    "image_labels_pkl = open(\"image_labels.pkl\",\"rb\")\n",
    "image_labels = pickle.load(image_labels_pkl)\n",
    "image_labels_pkl.close()\n",
    "\n",
    "groups_pkl = open(\"groups.pkl\",\"rb\")\n",
    "groups = pickle.load(groups_pkl)\n",
    "groups_pkl.close()\n",
    "\n",
    "\n",
    "# Unpickling unique image labels and data\n",
    "\n",
    "image_labels_unique_pkl = open(\"image_labels_unique.pkl\",\"rb\")\n",
    "image_labels_unique = pickle.load(image_labels_unique_pkl)\n",
    "image_labels_unique_pkl.close()\n",
    "\n",
    "image_data_unique_pkl = open(\"image_data_unique.pkl\",\"rb\")\n",
    "image_data_unique = pickle.load(image_data_unique_pkl)\n",
    "image_data_unique_pkl.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZaeA0vh21b3y",
    "nbpresent": {
     "id": "5ced1f54-9e37-472e-a833-297414ce5e53"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 135020/135020 [00:21<00:00, 6411.51it/s]\n"
     ]
    }
   ],
   "source": [
    "train_images = []\n",
    "train_questions = []\n",
    "train_answers = []\n",
    "train_groups = []\n",
    "\n",
    "for idx, val in enumerate(tqdm(image_labels)):\n",
    "    try:\n",
    "        u = image_labels_unique.index(val + \".png\")\n",
    "        v = image_data_unique[u]\n",
    "        train_images.append(v)\n",
    "        train_questions.append(questions[idx])\n",
    "        train_answers.append(answers[idx])\n",
    "        train_groups.append(groups[idx])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YU_JLxfJTThs",
    "nbpresent": {
     "id": "9d6d7c30-b50e-4183-8c31-ab01c11b1dc3"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135020, 60, 80, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images = np.array(train_images)\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ypUXz7pyn-7j",
    "nbpresent": {
     "id": "44e3f0cb-7036-4dd1-856c-e56ee3d0086f"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135020, 40)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_questions = np.array(train_questions)\n",
    "train_questions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HTRi_fhhY6e1"
   },
   "outputs": [],
   "source": [
    "unique_answers=list({i for i in train_answers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "spaBWxaJY6e4"
   },
   "outputs": [],
   "source": [
    "group = {'blue': 0,\n",
    " '2': 2,\n",
    " 'cylinder': 0,\n",
    " '5': 5,\n",
    " 'cyan': 1,\n",
    " 'purple': 2,\n",
    " '0': 0,\n",
    " 'False': 0,\n",
    " 'large': 0,\n",
    " '7': 7,\n",
    " 'True': 1,\n",
    " 'small': 1,\n",
    " '3': 3,\n",
    " 'cube': 1,\n",
    " '8': 8,\n",
    " 'sphere': 2,\n",
    " 'metal': 0,\n",
    " 'brown': 3,\n",
    " '4': 4,\n",
    " 'red': 4,\n",
    " '6': 6,\n",
    " 'gray': 5,\n",
    " '1': 1,\n",
    " 'green': 6,\n",
    " 'rubber': 1,\n",
    " 'yellow': 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TLACP7YjY6e7"
   },
   "outputs": [],
   "source": [
    "train_answers = [group[i] for i in train_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JbwCBeH6Y6e_"
   },
   "outputs": [],
   "source": [
    "test_images = train_images[:10000]\n",
    "test_questions = train_questions[:10000]\n",
    "test_answers = train_answers[:10000]\n",
    "test_groups = train_groups[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'False'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "umCDOW9eY6fD"
   },
   "outputs": [],
   "source": [
    "train_images = train_images[10000:]\n",
    "train_questions = train_questions[10000:]\n",
    "train_answers = train_answers[10000:]\n",
    "train_groups = train_groups[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wzbteoJaXyLq",
    "nbpresent": {
     "id": "39bf4551-1543-4c51-a95e-b52282ef56db"
    }
   },
   "source": [
    "### Model to get group from question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eXhEpTjMvot6",
    "nbpresent": {
     "id": "0865e2df-fbda-46f7-8ca7-30253a926ed3"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108016, 40) (108016, 6)\n",
      "(27004, 40) (27004, 6)\n"
     ]
    }
   ],
   "source": [
    "X = questions\n",
    "new_dict = {\n",
    "    'number': 0,\n",
    "    'material': 1,\n",
    "    'color': 2,\n",
    "    'shape': 3,\n",
    "    'size': 4,\n",
    "    'exist': 5\n",
    "}\n",
    "y = [new_dict[i] for i in groups]\n",
    "y = to_categorical(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v_Hv3f88vot-",
    "nbpresent": {
     "id": "3ae4f27b-b731-42fe-bd6a-5382184a9898"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 40, 100)           10000     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 128,022\n",
      "Trainable params: 128,022\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 100\n",
    "lstm_out = 128\n",
    "batch_size = 32\n",
    "\n",
    "inputs = Input((40, ))\n",
    "x = Embedding(100, embed_dim)(inputs)\n",
    "x = LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2)(x)\n",
    "x = Dense(6,activation='sigmoid')(x)\n",
    "model = Model(inputs, x)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZerT_AQkvouE",
    "nbpresent": {
     "id": "12638192-20b5-4dee-862a-9f5dba84e5f9"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rMdGX_NlvouG",
    "nbpresent": {
     "id": "bd6829af-8c72-4b98-9764-f081575651d2"
    }
   },
   "outputs": [],
   "source": [
    "early_stops = EarlyStopping(patience=3, monitor='val_acc')\n",
    "checkpointer = ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bB9WY25qvouJ",
    "nbpresent": {
     "id": "a8c1bce3-7ded-4c36-8631-f5fef1c99d0c"
    },
    "outputId": "74314915-b2e8-4aa8-8cc0-d34df57ab366"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 108016 samples, validate on 27004 samples\n",
      "Epoch 1/3\n",
      " - 240s - loss: 0.0815 - categorical_accuracy: 0.9678 - val_loss: 0.0011 - val_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00108, saving model to weights.hdf5\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-0fae2760b778>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size = batch_size, epochs = 3, callbacks=[checkpointer], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bPTRViyQT8dK",
    "nbpresent": {
     "id": "cc4aa421-8cdf-40c7-9141-87f247056d22"
    }
   },
   "source": [
    "### Get group from question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xXQbCX_0VXwp",
    "nbpresent": {
     "id": "39a192a1-8158-46a6-b146-863f7bc26a0a"
    }
   },
   "outputs": [],
   "source": [
    "questions_1 = ['There is a metal thing that is in front of the gray thing right of the big blue shiny sphere; how many rubber cubes are in front of it?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bHuwMtvoUvZb",
    "nbpresent": {
     "id": "a6bcbca5-34e1-4d58-9242-1f02576b305b"
    }
   },
   "outputs": [],
   "source": [
    "# tokenizer_1 = Tokenizer(num_words=100, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "#                 lower=True, split=' ')\n",
    "\n",
    "# tokenizer_1.fit_on_texts(questions_1)\n",
    "\n",
    "tokenizer_pkl = open(\"tokenizer.pkl\",\"rb\")\n",
    "tokenizer_1 = pickle.load(tokenizer_pkl)\n",
    "tokenizer_pkl.close()\n",
    "\n",
    "questions_tokenized_1 = tokenizer_1.texts_to_sequences(questions_1)\n",
    "questions_padded_1 = pad_sequences(questions_tokenized_1, maxlen=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g_F15Sx0UNiR",
    "nbpresent": {
     "id": "a4fe9916-99aa-4670-abe7-1bede3e60508"
    }
   },
   "outputs": [],
   "source": [
    "model = load_model('weights.hdf5')\n",
    "groups_1 = model.predict(questions_padded_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aXB_dmEbWDmK",
    "nbpresent": {
     "id": "c786a8e9-a459-4b59-904d-7d96a5642aaa"
    }
   },
   "outputs": [],
   "source": [
    "groups_1 = np.argmax(groups_1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fNtpADuCWO-g",
    "nbpresent": {
     "id": "1b671606-8e87-48e9-8d8f-80da10587943"
    },
    "outputId": "6777cc8d-8879-4f9e-a5d4-319118bac9a1"
   },
   "outputs": [],
   "source": [
    "groups_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BFATxN4NWgjg",
    "nbpresent": {
     "id": "3ad6938a-55a6-49b9-b508-90551e6b12b8"
    },
    "outputId": "d3df8e07-7507-4c78-e321-a897e7696829"
   },
   "outputs": [],
   "source": [
    "for i in groups_1:\n",
    "    for key, value in new_dict.items():\n",
    "        if value == i:\n",
    "            print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fIXIBu_tY6f3"
   },
   "source": [
    "### Split into groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EEQlOYaxY6f5"
   },
   "outputs": [],
   "source": [
    "answer_types = ['number','material','color','shape','size','exist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8rbAqMpTY6f8"
   },
   "outputs": [],
   "source": [
    "train_questions_grouped = {}\n",
    "train_answers_grouped = {}\n",
    "train_images_grouped = {}\n",
    "\n",
    "test_questions_grouped = {}\n",
    "test_answers_grouped = {}\n",
    "test_images_grouped = {}\n",
    "\n",
    "for group in answer_types:\n",
    "    train_questions_grouped[group] = []\n",
    "    train_answers_grouped[group] = []\n",
    "    train_images_grouped[group] = []\n",
    "    \n",
    "    test_questions_grouped[group] = []\n",
    "    test_answers_grouped[group] = []\n",
    "    test_images_grouped[group] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aiojTVNnY6f-",
    "outputId": "d402156f-0683-4c70-fdac-42f7fa483ca1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 125020/125020 [00:00<00:00, 631418.09it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 666683.20it/s]\n"
     ]
    }
   ],
   "source": [
    "for idx, value in enumerate(tqdm(train_groups)):\n",
    "    train_questions_grouped[value].append(train_questions[idx])\n",
    "    train_answers_grouped[value].append(train_answers[idx])\n",
    "    train_images_grouped[value].append(train_images[idx])\n",
    "    \n",
    "for idx, value in enumerate(tqdm(test_groups)):\n",
    "    test_questions_grouped[value].append(test_questions[idx])\n",
    "    test_answers_grouped[value].append(test_answers[idx])\n",
    "    test_images_grouped[value].append(test_images[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KUFatqswY6gD"
   },
   "outputs": [],
   "source": [
    "for key, val in train_answers_grouped.items():\n",
    "    train_answers_grouped[key] = to_categorical(val)\n",
    "\n",
    "for key, val in test_answers_grouped.items():\n",
    "    test_answers_grouped[key] = to_categorical(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YvXv01W2Y6gL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number 29457\n",
      "material 11248\n",
      "color 11250\n",
      "shape 11250\n",
      "size 11247\n",
      "exist 50568\n",
      "number 2356\n",
      "material 900\n",
      "color 900\n",
      "shape 900\n",
      "size 900\n",
      "exist 4044\n"
     ]
    }
   ],
   "source": [
    "for key, val in train_questions_grouped.items():\n",
    "    print(key, len(val))\n",
    "    \n",
    "for key, val in test_questions_grouped.items():\n",
    "    print(key, len(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZHoGzMqkvouZ",
    "nbpresent": {
     "id": "7a22464d-9db5-4615-b26a-9190a27664f6"
    }
   },
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PBW_7O5ANCMB",
    "nbpresent": {
     "id": "fcb6ddd4-154d-4416-a8a2-beec4927fcc8"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers.convolutional import Conv2D\n",
    "\n",
    "def ConvolutionNetworks(kernel_size=3, stride_size=2):\n",
    "    def conv(model):\n",
    "        model = Conv2D(24, (5, 5), strides=(stride_size, stride_size),activation='relu',input_shape=(60, 80, 3), data_format='channels_last')(model)\n",
    "        model = BatchNormalization()(model)\n",
    "        model = Conv2D(24, (5, 5), strides=(stride_size, stride_size),activation='relu')(model)\n",
    "        model = BatchNormalization()(model)\n",
    "        model = Conv2D(24, (kernel_size, kernel_size), strides=(stride_size, stride_size),activation='relu')(model)\n",
    "        model = BatchNormalization()(model)\n",
    "        model = Conv2D(24, (3, 3), strides=(1, 1),activation='relu')(model)\n",
    "        model = BatchNormalization()(model)\n",
    "        return model\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMN_pjQDNqgo",
    "nbpresent": {
     "id": "e2994340-2a82-4886-ade5-827785424ee3"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Lambda, Concatenate, Add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "def slicer(x_loc, y_loc):\n",
    "    def func(x):\n",
    "        return x[:,x_loc,y_loc,:]\n",
    "    return Lambda(func)\n",
    "\n",
    "def object_tagging(o, i, d):\n",
    "    coor = K.variable(value=[float(int(i/d))/d*2-1, float((i%d))/d*2-1])\n",
    "    coor = K.expand_dims(coor, axis=0)\n",
    "    batch_size = K.shape(o)[0]\n",
    "    coor = K.tile(coor, [batch_size, 1])\n",
    "    coor = Input(tensor=coor)\n",
    "    o = Concatenate()([coor, o])\n",
    "    return o\n",
    "    \n",
    "def compute_relations(objects, question):\n",
    "    \n",
    "    def get_top_dim_1(t):\n",
    "        return t[:, 0, :, :]\n",
    "\n",
    "    def get_all_but_top_dim_1(t):\n",
    "        return t[:, 1:, :, :]\n",
    "\n",
    "    def get_top_dim_2(t):\n",
    "        return t[:, 0, :]\n",
    "\n",
    "    def get_all_but_top_dim2(t):\n",
    "        return t[:, 1:, :]\n",
    "    \n",
    "    slice_top_dim_1 = Lambda(get_top_dim_1)\n",
    "    slice_all_but_top_dim_1 = Lambda(get_all_but_top_dim_1)\n",
    "    slice_top_dim_2 = Lambda(get_top_dim_2)\n",
    "    slice_all_but_top_dim2 = Lambda(get_all_but_top_dim2)\n",
    "    \n",
    "    d = K.int_shape(objects)[2]\n",
    "    features = []\n",
    "    for i in range(d):\n",
    "        features1 = slice_top_dim_1(objects)\n",
    "        objects = slice_all_but_top_dim_1(objects)\n",
    "        for j in range(d):\n",
    "            features2 = slice_top_dim_2(features1)\n",
    "            features1 = slice_all_but_top_dim2(features1)\n",
    "            features.append(features2)\n",
    "    \n",
    "    relations = []\n",
    "    concat = Concatenate()\n",
    "    for feature1 in features:\n",
    "        for feature2 in features:\n",
    "            relations.append(concat([feature1, feature2, question]))\n",
    "    \n",
    "\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xKdKH33RQHf5",
    "nbpresent": {
     "id": "186bedbb-ef50-4a95-a41e-f1826544f346"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "def f_theta():\n",
    "    def f(model):\n",
    "        model = Dense(256)(model)\n",
    "        model = Activation('relu')(model)\n",
    "        model = Dense(256)(model)\n",
    "        model = Activation('relu')(model)\n",
    "        model = Dense(256)(model)\n",
    "        model = Activation('relu')(model)\n",
    "        model = Dense(256)(model)\n",
    "        model = Activation('relu')(model)\n",
    "        return model\n",
    "    return f\n",
    "\n",
    "baseline_scene = Input((60, 80, 3))\n",
    "baseline_question = Input((40,))\n",
    "baseline_conv = ConvolutionNetworks()(baseline_scene)\n",
    "baseline_conv = Flatten()(baseline_conv)\n",
    "baseline_conv = Concatenate()([baseline_conv, baseline_question])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-d5vVEPY6gm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['number', 'material', 'color', 'shape', 'size', 'exist']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vpcvrr7DY6go"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'number': 9, 'material': 2, 'color': 8, 'shape': 3, 'size': 2, 'exist': 2}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_groups = {}\n",
    "for key, val in train_answers_grouped.items():\n",
    "    length_groups[key] = len(train_answers_grouped[key][0])\n",
    "length_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v7yz3MjHD0js",
    "nbpresent": {
     "id": "de466352-fd5d-4452-8884-b0dfb3a0825f"
    }
   },
   "source": [
    "### Train Baseline Moodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pufkBSouY6gs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 10.43it/s]\n"
     ]
    }
   ],
   "source": [
    "models_grouped = {}\n",
    "\n",
    "for group in tqdm(answer_types):\n",
    "    baseline_output = f_theta()(baseline_conv) \n",
    "    baseline_output = Dense(length_groups[group], activation='softmax')(baseline_output)\n",
    "    baseline_model = Model(inputs=[baseline_scene, baseline_question], outputs=baseline_output)\n",
    "    baseline_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    models_grouped[group] = baseline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z5k9SdZ0Y6gv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number\n",
      "Train on 29457 samples, validate on 2356 samples\n",
      "Epoch 1/5\n",
      " - 10s - loss: 1.4605 - acc: 0.3581 - val_loss: 1.4081 - val_acc: 0.3833\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.40805, saving model to number.hdf5\n",
      "Epoch 2/5\n",
      " - 8s - loss: 1.3737 - acc: 0.3804 - val_loss: 1.3810 - val_acc: 0.3820\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.40805 to 1.38099, saving model to number.hdf5\n",
      "Epoch 3/5\n",
      " - 8s - loss: 1.3311 - acc: 0.3937 - val_loss: 1.3613 - val_acc: 0.3765\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.38099 to 1.36131, saving model to number.hdf5\n",
      "Epoch 4/5\n",
      " - 8s - loss: 1.2902 - acc: 0.4076 - val_loss: 1.3730 - val_acc: 0.3782\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.36131\n",
      "Epoch 5/5\n",
      " - 8s - loss: 1.2484 - acc: 0.4235 - val_loss: 1.3778 - val_acc: 0.3973\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.36131\n",
      "material\n",
      "Train on 11248 samples, validate on 900 samples\n",
      "Epoch 1/5\n",
      " - 4s - loss: 0.7311 - acc: 0.4941 - val_loss: 0.6985 - val_acc: 0.4822\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69852, saving model to material.hdf5\n",
      "Epoch 2/5\n",
      " - 3s - loss: 0.6921 - acc: 0.5295 - val_loss: 0.7005 - val_acc: 0.5056\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.69852\n",
      "Epoch 3/5\n",
      " - 3s - loss: 0.6811 - acc: 0.5674 - val_loss: 0.7137 - val_acc: 0.4967\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.69852\n",
      "Epoch 4/5\n",
      " - 3s - loss: 0.6601 - acc: 0.6006 - val_loss: 0.7136 - val_acc: 0.5222\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.69852\n",
      "Epoch 5/5\n",
      " - 3s - loss: 0.6235 - acc: 0.6461 - val_loss: 0.7568 - val_acc: 0.5311\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.69852\n",
      "color\n",
      "Train on 11250 samples, validate on 900 samples\n",
      "Epoch 1/5\n",
      " - 4s - loss: 2.1441 - acc: 0.1236 - val_loss: 2.0873 - val_acc: 0.1211\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.08732, saving model to color.hdf5\n",
      "Epoch 2/5\n",
      " - 3s - loss: 2.0733 - acc: 0.1509 - val_loss: 2.0898 - val_acc: 0.1433\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.08732\n",
      "Epoch 3/5\n",
      " - 3s - loss: 2.0542 - acc: 0.1755 - val_loss: 2.1171 - val_acc: 0.1222\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.08732\n",
      "Epoch 4/5\n",
      " - 3s - loss: 2.0087 - acc: 0.2137 - val_loss: 2.1548 - val_acc: 0.1122\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.08732\n",
      "Epoch 5/5\n",
      " - 3s - loss: 1.9244 - acc: 0.2637 - val_loss: 2.1908 - val_acc: 0.1456\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.08732\n",
      "shape\n",
      "Train on 11250 samples, validate on 900 samples\n",
      "Epoch 1/5\n",
      " - 5s - loss: 1.1918 - acc: 0.3319 - val_loss: 1.1131 - val_acc: 0.3167\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.11312, saving model to shape.hdf5\n",
      "Epoch 2/5\n",
      " - 3s - loss: 1.0939 - acc: 0.3758 - val_loss: 1.1130 - val_acc: 0.3444\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.11312 to 1.11301, saving model to shape.hdf5\n",
      "Epoch 3/5\n",
      " - 3s - loss: 1.0733 - acc: 0.4206 - val_loss: 1.1273 - val_acc: 0.3256\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.11301\n",
      "Epoch 4/5\n",
      " - 3s - loss: 1.0324 - acc: 0.4700 - val_loss: 1.1676 - val_acc: 0.3400\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.11301\n",
      "Epoch 5/5\n",
      " - 3s - loss: 0.9590 - acc: 0.5340 - val_loss: 1.2171 - val_acc: 0.3678\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.11301\n",
      "size\n",
      "Train on 11247 samples, validate on 900 samples\n",
      "Epoch 1/5\n",
      " - 5s - loss: 0.7350 - acc: 0.4980 - val_loss: 0.7226 - val_acc: 0.4978\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.72258, saving model to size.hdf5\n",
      "Epoch 2/5\n",
      " - 3s - loss: 0.6931 - acc: 0.5354 - val_loss: 0.6936 - val_acc: 0.5144\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.72258 to 0.69356, saving model to size.hdf5\n",
      "Epoch 3/5\n",
      " - 3s - loss: 0.6803 - acc: 0.5655 - val_loss: 0.6986 - val_acc: 0.5144\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.69356\n",
      "Epoch 4/5\n",
      " - 3s - loss: 0.6488 - acc: 0.6145 - val_loss: 0.7214 - val_acc: 0.5211\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.69356\n",
      "Epoch 5/5\n",
      " - 3s - loss: 0.5949 - acc: 0.6744 - val_loss: 0.7638 - val_acc: 0.4922\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.69356\n",
      "exist\n",
      "Train on 50568 samples, validate on 4044 samples\n",
      "Epoch 1/5\n",
      " - 15s - loss: 0.7098 - acc: 0.5057 - val_loss: 0.7013 - val_acc: 0.4886\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.70131, saving model to exist.hdf5\n",
      "Epoch 2/5\n",
      " - 14s - loss: 0.6929 - acc: 0.5190 - val_loss: 0.6957 - val_acc: 0.5015\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.70131 to 0.69573, saving model to exist.hdf5\n",
      "Epoch 3/5\n",
      " - 14s - loss: 0.6899 - acc: 0.5290 - val_loss: 0.6940 - val_acc: 0.5176\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.69573 to 0.69401, saving model to exist.hdf5\n",
      "Epoch 4/5\n",
      " - 13s - loss: 0.6846 - acc: 0.5461 - val_loss: 0.6928 - val_acc: 0.5141\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.69401 to 0.69278, saving model to exist.hdf5\n",
      "Epoch 5/5\n",
      " - 14s - loss: 0.6734 - acc: 0.5725 - val_loss: 0.7084 - val_acc: 0.5089\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.69278\n"
     ]
    }
   ],
   "source": [
    "trained_models_grouped = {}\n",
    "\n",
    "for group in answer_types:\n",
    "    print(group)\n",
    "    early_stops = EarlyStopping(patience=2, monitor='val_acc')\n",
    "    checkpointer = ModelCheckpoint(filepath=group+'.hdf5', verbose=1, save_best_only=True)\n",
    "    trained_models_grouped[group] = models_grouped[group].fit([train_images_grouped[group], train_questions_grouped[group]], train_answers_grouped[group], validation_data=([test_images_grouped[group], test_questions_grouped[group]], test_answers_grouped[group]), epochs = 5, shuffle = True, batch_size = 50, callbacks=[checkpointer], verbose=2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "D1rQ5a-SFOOG",
    "WG3R9I_b85DG"
   ],
   "name": "NNFL-SplitModels.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
